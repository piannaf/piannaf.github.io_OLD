<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    
    <!-- Always force latest IE rendering engine or request Chrome Frame -->
    <meta content="IE=edge,chrome=1" http-equiv="X-UA-Compatible">
    
    <!-- Use title if it's in the page YAML frontmatter -->
    <title>
      Justin Mancinelli -
      Freeing Data from PDFs
    </title>
    <link href='http://fonts.googleapis.com/css?family=Lato:400,700' rel='stylesheet' type='text/css'>
    <link href="/stylesheets/all.css" rel="stylesheet" type="text/css" /><link href="/stylesheets/global-header.css" rel="stylesheet" type="text/css" /><link href="/stylesheets/article.css" rel="stylesheet" type="text/css" />
    <script src="/javascripts/all.js" type="text/javascript"></script>
  </head>
  
  <body class="article article_2015 article_2015_06 article_2015_06_21 article_2015_06_21_freeing-data-from-pdfs">
    <div>
      <header id="header">
        <h1><a href="/">Justin Mancinelli</a></h1>
<h2>Agile developer, coach, and leader</h2>

      </header>
    </div>
    <article>
      <header style="background-image:url(/images/rgb2csv.png)">
        <div>
          <h1>Freeing Data from PDFs</h1>
          <h2>Jun 21, 2015</h2>
        </div>
      </header>
      <section>
        <div>
          <p>I wanted to process some data for stabilised rents in NYC but the best I could find was in PDFs. Why do so many governments lock away their data in PDFs?</p>

<p></p>

<p>Here is the story behind my project <a href="https://github.com/piannaf/rgb2csv">rgb2csv</a>.</p>

<h1 id="the-rant">The Rant</h1>

<p>Don’t get me wrong, I’m happy the data is out on the internet (even if it is outdated, not accurate, and only partially relevant). It’s better than putting in a formal request and then waiting a month or worse: needing to physically go see a clerk and ask for some print-outs.</p>

<p>The problem with PDFs is they are almost as bad a print-outs. They are called PDFs for a reason. They are an outstanding format for creating portable documents. However, data should’t be written in document form any more. Data over the internet is portable when it is accessible from an API.</p>

<h1 id="the-project">The Project</h1>

<p>In my search for New York City stabilised rent data, most signs pointed to the NYC Rent Guidelines Board who made <a href="http://www.nycrgb.org/html/resources/zip.html">Rent Stabilised Buildings Listing data</a>, from 2013, available for download as a PDF for each burrough of NYC.</p>

<p>My first plan was to</p>

<ol>
  <li>Extract the data from PDF into CSV because CSVs are super easy to process</li>
  <li>Process the data into a form I cared about and move it into a database</li>
  <li>Write an API to access the data from the database</li>
  <li>Create a website that pulls data from the API, maps the building locations and provides more info</li>
</ol>

<p>It was by coincidence that, not much earlier, <a href="https://twitter.com/noruweijin">Havard Ferstad</a> had pointed me to <a href="https://github.com/hhtyo/great-east-japan-earthquake-evacuees">Great East Japan Earthquake Evacuees</a>, a project extracting data from PDFs sourced from Japan's Reconstruction Agency. Now I had a starting point.</p>

<p><a href="https://github.com/tabulapdf/tabula-extractor">Tabula-extractor</a> is written for JRuby because it makes use of <a href="https://pdfbox.apache.org/">PDFBox</a> which is written in Java. To keep things simple, I started off by interfacing with tabula-extractor via a Ruby script. I named my script <a href="https://github.com/piannaf/rgb2csv">rgb2csv</a> where “rgb” is a reference to the Rent Guidelines Board’s PDFs and “csv” is the output format of the script.</p>

<p>After completing step 1 of my plan for Manhattan only, I discovered other people’s projects which had already completed mine in a much better way. I’ll introduce them later.</p>

<h1 id="the-implementation">The Implementation</h1>

<p><a href="https://github.com/tabulapdf/tabula-extractor">Tabula-extractor</a> is very easy to work with because it turns the chore of scraping a PDF into a very declarative experience.</p>

<p>The entry function calls on tabula-extractor to do all the grunt-work of paginating the PDF into useful objects. The only real logic I needed to write was to clean up the data and combine it into a single CSV.</p>

<pre><code>def extract_data_from(pdf_file)
  extractor = Tabula::Extraction::ObjectExtractor.new(pdf_file, 1..2 )
  tables = extractor.extract.map { |page|
    table_area = select_table_area_from(page)
    table = extract_table_from(table_area)
    cleanup(table)                                                              
  }
  extractor.close!

  combine(tables)
end
</code></pre>

<p>The automated table detector wasn’t able to work because the PDF was a mix of right, left, and center aligned columns and many columns had very sparse data. I had to manually inspect the column spacing which means this script is fragile. I have some ideas that would make this unnecessary but won’t be trying them out since I ended this project.</p>

<pre><code>def extract_table_from(table_area)
  vertical_rulings = [50,103,198,245,280,370,400,450,542,630,703,727].map{ |n|
    Tabula::Ruling.new(0, n, 0, 1000)
  }

  table_area.get_table(:vertical_rulings =&gt; vertical_rulings)
end
</code></pre>

<p>Cleaning up was necessary because there were strange characters showing up instead of whitespace between words. On closer inspection, they were whitespace characters so I could just get rid of them by searching for whitespace. This is a quick fix though, so if I were continuing this project I’d want to test more.</p>

<pre><code>def cleanup(table)
  table.rows.map {|row|
    # extract text from text elements
    # and remove some strange whitespace
    row.map { |te| te.text.gsub(/\s+/, '').strip }
  }
end
</code></pre>

<p>Finally, combining all the tables then outputting the CSV was a simple matter of keeping the header row unique to the entire table rather than on each page then joining all the lines together and ensuring the output ended with a new line.</p>

<pre><code>def combine(tables)
  # All tables have the same header
  # Ensure it stays a line of its own so we can concat to the rest
  header = [tables[0][0]]

  body = tables.map { |table| table[1..-1] }.flatten(1)

  header.concat(body)
end

def csv_from(data)
  data.map { |line| line.join(",") }.join("\n") + "\n”
end
</code></pre>

<h1 id="the-other-peoples-projects">The Other People’s Projects</h1>

<p>I have no more plans to update or maintain this program because I discovered the following projects:</p>

<ul>
  <li><a href="https://github.com/clhenrick/dhcr-rent-stabilized-data">DHCR Rent Stabilized Data</a> which got Excel spreadsheets straight from <a href="http://www.nyshcr.org/">DHCR</a> and made a <a href="https://chenrick.cartodb.com/viz/20b7c6ac-ee12-11e4-b74e-0e853d047bba/public_map">beautiful map</a></li>
  <li><a href="https://github.com/talos/nyc-stabilization-unit-counts">NYC Stabilization Unit Counts</a> which is a massive PDF scraping project of NYC Tax Bills.</li>
</ul>

<p>There is tremendous work done above. I'd like to mashup their data with others sometime.</p>


        </div>
      </section>
    </article>
    <script type="text/javascript">
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-49284511-1', 'auto');
  ga('send', 'pageview');
</script>
  </body>
</html>
