<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Blog Name</title>
  <subtitle>Blog subtitle</subtitle>
  <id>http://blog.url.com/</id>
  <link href="http://blog.url.com/"/>
  <link href="http://blog.url.com/feed.xml" rel="self"/>
  <updated>2015-06-21T00:00:00+00:00</updated>
  <author>
    <name>Blog Author</name>
  </author>
  <entry>
    <title>Freeing Data from PDFs</title>
    <link rel="alternate" href="http://blog.url.com/article/2015/06/21/freeing-data-from-pdfs.html"/>
    <id>http://blog.url.com/article/2015/06/21/freeing-data-from-pdfs.html</id>
    <published>2015-06-21T00:00:00+00:00</published>
    <updated>2015-07-13T10:08:02+00:00</updated>
    <author>
      <name>Article Author</name>
    </author>
    <content type="html">&lt;p&gt;I wanted to process some data for stabilised rents in NYC but the best I could find was in PDFs. Why do so many governments lock away their data in PDFs?&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Here is the story behind my project &lt;a href="https://github.com/piannaf/rgb2csv"&gt;rgb2csv&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id="the-rant"&gt;The Rant&lt;/h1&gt;

&lt;p&gt;Don’t get me wrong, I’m happy the data is out on the internet (even if it is outdated, not accurate, and only partially relevant). It’s better than putting in a formal request and then waiting a month or worse: needing to physically go see a clerk and ask for some print-outs.&lt;/p&gt;

&lt;p&gt;The problem with PDFs is they are almost as bad a print-outs. They are called PDFs for a reason. They are an outstanding format for creating portable documents. However, data should’t be written in document form any more. Data over the internet is portable when it is accessible from an API.&lt;/p&gt;

&lt;h1 id="the-project"&gt;The Project&lt;/h1&gt;

&lt;p&gt;In my search for New York City stabilised rent data, most signs pointed to the NYC Rent Guidelines Board who made &lt;a href="http://www.nycrgb.org/html/resources/zip.html"&gt;Rent Stabilised Buildings Listing data&lt;/a&gt;, from 2013, available for download as a PDF for each burrough of NYC.&lt;/p&gt;

&lt;p&gt;My first plan was to&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Extract the data from PDF into CSV because CSVs are super easy to process&lt;/li&gt;
  &lt;li&gt;Process the data into a form I cared about and move it into a database&lt;/li&gt;
  &lt;li&gt;Write an API to access the data from the database&lt;/li&gt;
  &lt;li&gt;Create a website that pulls data from the API, maps the building locations and provides more info&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;It was by coincidence that, not much earlier, &lt;a href="https://twitter.com/noruweijin"&gt;Havard Ferstad&lt;/a&gt; had pointed me to &lt;a href="https://github.com/hhtyo/great-east-japan-earthquake-evacuees"&gt;Great East Japan Earthquake Evacuees&lt;/a&gt;, a project extracting data from PDFs sourced from Japan's Reconstruction Agency. Now I had a starting point.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://github.com/tabulapdf/tabula-extractor"&gt;Tabula-extractor&lt;/a&gt; is written for JRuby because it makes use of &lt;a href="https://pdfbox.apache.org/"&gt;PDFBox&lt;/a&gt; which is written in Java. To keep things simple, I started off by interfacing with tabula-extractor via a Ruby script. I named my script &lt;a href="https://github.com/piannaf/rgb2csv"&gt;rgb2csv&lt;/a&gt; where “rgb” is a reference to the Rent Guidelines Board’s PDFs and “csv” is the output format of the script.&lt;/p&gt;

&lt;p&gt;After completing step 1 of my plan for Manhattan only, I discovered other people’s projects which had already completed mine in a much better way. I’ll introduce them later.&lt;/p&gt;

&lt;h1 id="the-implementation"&gt;The Implementation&lt;/h1&gt;

&lt;p&gt;&lt;a href="https://github.com/tabulapdf/tabula-extractor"&gt;Tabula-extractor&lt;/a&gt; is very easy to work with because it turns the chore of scraping a PDF into a very declarative experience.&lt;/p&gt;

&lt;p&gt;The entry function calls on tabula-extractor to do all the grunt-work of paginating the PDF into useful objects. The only real logic I needed to write was to clean up the data and combine it into a single CSV.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def extract_data_from(pdf_file)
  extractor = Tabula::Extraction::ObjectExtractor.new(pdf_file, 1..2 )
  tables = extractor.extract.map { |page|
    table_area = select_table_area_from(page)
    table = extract_table_from(table_area)
    cleanup(table)                                                              
  }
  extractor.close!

  combine(tables)
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The automated table detector wasn’t able to work because the PDF was a mix of right, left, and center aligned columns and many columns had very sparse data. I had to manually inspect the column spacing which means this script is fragile. I have some ideas that would make this unnecessary but won’t be trying them out since I ended this project.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def extract_table_from(table_area)
  vertical_rulings = [50,103,198,245,280,370,400,450,542,630,703,727].map{ |n|
    Tabula::Ruling.new(0, n, 0, 1000)
  }

  table_area.get_table(:vertical_rulings =&amp;gt; vertical_rulings)
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cleaning up was necessary because there were strange characters showing up instead of whitespace between words. On closer inspection, they were whitespace characters so I could just get rid of them by searching for whitespace. This is a quick fix though, so if I were continuing this project I’d want to test more.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def cleanup(table)
  table.rows.map {|row|
    # extract text from text elements
    # and remove some strange whitespace
    row.map { |te| te.text.gsub(/\s+/, '').strip }
  }
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, combining all the tables then outputting the CSV was a simple matter of keeping the header row unique to the entire table rather than on each page then joining all the lines together and ensuring the output ended with a new line.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def combine(tables)
  # All tables have the same header
  # Ensure it stays a line of its own so we can concat to the rest
  header = [tables[0][0]]

  body = tables.map { |table| table[1..-1] }.flatten(1)

  header.concat(body)
end

def csv_from(data)
  data.map { |line| line.join(",") }.join("\n") + "\n”
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id="the-other-peoples-projects"&gt;The Other People’s Projects&lt;/h1&gt;

&lt;p&gt;I have no more plans to update or maintain this program because I discovered the following projects:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href="https://github.com/clhenrick/dhcr-rent-stabilized-data"&gt;DHCR Rent Stabilized Data&lt;/a&gt; which got Excel spreadsheets straight from &lt;a href="http://www.nyshcr.org/"&gt;DHCR&lt;/a&gt; and made a &lt;a href="https://chenrick.cartodb.com/viz/20b7c6ac-ee12-11e4-b74e-0e853d047bba/public_map"&gt;beautiful map&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://github.com/talos/nyc-stabilization-unit-counts"&gt;NYC Stabilization Unit Counts&lt;/a&gt; which is a massive PDF scraping project of NYC Tax Bills.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There is tremendous work done above. I'd like to mashup their data with others sometime.&lt;/p&gt;

</content>
  </entry>
  <entry>
    <title>Checksum != Security</title>
    <link rel="alternate" href="http://blog.url.com/article/2015/03/17/checksum-security.html"/>
    <id>http://blog.url.com/article/2015/03/17/checksum-security.html</id>
    <published>2015-03-17T00:00:00+00:00</published>
    <updated>2015-07-22T04:39:01+00:00</updated>
    <author>
      <name>Article Author</name>
    </author>
    <content type="html">&lt;p&gt;One day I stumbled upon a web app testing environment that used client side Javascript to perform authentication. &lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;It was very simple to break into because it hashed the password using a very simple checksum algorithm. &lt;/p&gt;

&lt;p&gt;I created &lt;a href="http://www.slideshare.net/JustinMancinelli/checksums-are-not-secure"&gt;this presentation&lt;/a&gt; to share my thoughts on what I found.&lt;/p&gt;

&lt;script async="" class="speakerdeck-embed" data-id="2cc5342a058d4b6483ff0edf97193957" data-ratio="1.33333333333333" src="//speakerdeck.com/assets/embed.js"&gt;&lt;/script&gt;

</content>
  </entry>
  <entry>
    <title>Explode Tabs</title>
    <link rel="alternate" href="http://blog.url.com/article/2011/02/09/explode-tabs.html"/>
    <id>http://blog.url.com/article/2011/02/09/explode-tabs.html</id>
    <published>2011-02-09T00:00:00+00:00</published>
    <updated>2015-07-06T07:03:07+00:00</updated>
    <author>
      <name>Article Author</name>
    </author>
    <content type="html">&lt;p&gt;My first Chrome extension isn't very useful as an extension but it was useful
for learning.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;I took the (very useful) &lt;a href="https://chrome.google.com/extensions/detail/adjadgadeebehakpgamlnafmdkegkmph"&gt;Merge Windows&lt;/a&gt;
extension and did the opposite, hence, Explode Tabs.&lt;/p&gt;

&lt;p&gt;You can view the source by &lt;a href="https://chrome.google.com/extensions/detail/gcinfingfgnnjglljnedbhlpobmgagpp"&gt;installing it&lt;/a&gt;
then opening up the file.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Database Usage Analytics</title>
    <link rel="alternate" href="http://blog.url.com/article/2010/12/02/database-usage-analytics.html"/>
    <id>http://blog.url.com/article/2010/12/02/database-usage-analytics.html</id>
    <published>2010-12-02T00:00:00+00:00</published>
    <updated>2015-07-06T07:03:07+00:00</updated>
    <author>
      <name>Article Author</name>
    </author>
    <content type="html">&lt;p&gt;The bulk of this assigment focused on writing PL/SQL (this was done for Oracle)
to analyse usage of the database including users, queries, and indexes.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;The course for this assignment was about being a database administrator, not a
Database designer. Database design was the focus of the information systems
class where I designed and implemented a
&lt;a href="/article/2009/11/06/mobile-content-information-system.html"&gt;Mobile Content Information System&lt;/a&gt;.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Encrypted File System</title>
    <link rel="alternate" href="http://blog.url.com/article/2010/11/29/encrypted-file-system.html"/>
    <id>http://blog.url.com/article/2010/11/29/encrypted-file-system.html</id>
    <published>2010-11-29T00:00:00+00:00</published>
    <updated>2015-07-06T07:03:07+00:00</updated>
    <author>
      <name>Article Author</name>
    </author>
    <content type="html">&lt;p&gt;Talk about a deceptively simple assignment. Two parts: implement
‘immediate files’ and ‘encryption’.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;There were times when a single line of code took hours to research. The lecture
content was basically: there exists a VFS and data is stored in blocks. I
walked the &lt;a href="http://lxr.linux.no/linux+v2.6.36/fs/"&gt;Linux source tree&lt;/a&gt; down from
the open and write system calls through the VFS layer to the block device
driver to understand this stuff and it was a lot of fun.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Encryption Device Driver</title>
    <link rel="alternate" href="http://blog.url.com/article/2010/11/28/encryption-device-driver.html"/>
    <id>http://blog.url.com/article/2010/11/28/encryption-device-driver.html</id>
    <published>2010-11-28T00:00:00+00:00</published>
    <updated>2015-07-06T07:03:07+00:00</updated>
    <author>
      <name>Article Author</name>
    </author>
    <content type="html">&lt;p&gt;We had to implement a linux device driver interface to an encryption
‘coprocessor’. The driver performs a XOR-based stream cipher since we didn’t
have special encryption hardware.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;By far, the most interesting and most fun assignment I’ve ever had. The
lectures consisted of just about zero information about how to go about this so
there was a fair bit of research to do. &lt;/p&gt;

&lt;p&gt;I decided to make full use of the Linux kernel &lt;a href="http://lxr.linux.no/linux+v2.6.36/include/linux/list.h"&gt;linked list&lt;/a&gt;
and &lt;a href="http://lxr.linux.no/linux+v2.6.36/include/linux/circ_buf.h"&gt;circular buffer&lt;/a&gt;
macros.&lt;/p&gt;
</content>
  </entry>
</feed>
